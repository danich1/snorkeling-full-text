{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Import for Entire Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T18:34:28.360662Z",
     "start_time": "2021-01-06T18:34:26.691642Z"
    }
   },
   "outputs": [],
   "source": [
    "## Default Libraries\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "import csv\n",
    "import dill as pickle\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "## External Libraries\n",
    "import intervaltree\n",
    "from joblib import Parallel, delayed\n",
    "import lxml.etree as ET\n",
    "from multiprocessing import  Process, Manager\n",
    "import pandas as pd\n",
    "from sqlalchemy import (\n",
    "    create_engine, Integer,\n",
    "    SmallInteger, BigInteger,\n",
    "    MetaData, Table,\n",
    "    String, VARCHAR,\n",
    "    ARRAY, Column,\n",
    "    Index, ForeignKey,\n",
    "    UniqueConstraint\n",
    ")\n",
    "import tqdm\n",
    "\n",
    "## Custom module for pipeline\n",
    "from snorkeling_helper.database_helper import (\n",
    "    insert_entities,\n",
    "    get_sql_result,\n",
    "    parse_document, \n",
    "    supply_documents,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Insertion pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to import all tagged text from Pubtator Central into a Postgres database. This notebook can take at least a week to run, so be patient as large NLP projects can take a gargantuan amount of time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section invokes sqlalchemy's connection module to create a database called pubmed_central_db and populate the database with the following table declaration. The database name can be changed to whichever name you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"####\" # replace with personal postgres username\n",
    "password = \"#####\" # replace with personal postgres password\n",
    "dbname = \"pubmed_central_db\"\n",
    "database_str = f\"postgresql+psycopg2://{username}:{password}@/{dbname}?host=/var/run/postgresql\"\n",
    "conn = create_engine(database_str)\n",
    "\n",
    "metadata = MetaData()\n",
    "sentence = Table(\n",
    "    'sentence', metadata,\n",
    "    Column('sentence_id', BigInteger, primary_key=True),\n",
    "    Column('document_id', Integer),\n",
    "    Column('section', String),\n",
    "    Column('position', SmallInteger),\n",
    "    Column('text', String),\n",
    "    Column('word', String),\n",
    "    Column('pos_tag', String),\n",
    "    Column('lemma', String),\n",
    "    Column('dep', String),\n",
    "    Column('char_offset', String),\n",
    "    Index('ix_sentence_document_id', 'document_id'),\n",
    "    UniqueConstraint(\"document_id\", \"section\", \"position\", name=\"sentence_integrity\")\n",
    ")\n",
    "\n",
    "entity = Table(\n",
    "    'entity', metadata,\n",
    "    Column('entity_id', BigInteger, primary_key=True),\n",
    "    Column('document_id', Integer),\n",
    "    Column('entity_type', VARCHAR),\n",
    "    Column('entity_cids', VARCHAR),\n",
    "    Column('start', Integer),\n",
    "    Column('end', Integer),\n",
    "    Index('ix_entity_document_id', 'document_id'),\n",
    "    UniqueConstraint(\"document_id\", \"start\", \"end\", name=\"entity_integrity\")\n",
    ")\n",
    "\n",
    "candidate = Table(\n",
    "    'candidate', metadata,\n",
    "    Column('candidate_id', BigInteger, primary_key=True),\n",
    "    Column('candidate_type', VARCHAR),\n",
    "    Column('dataset', SmallInteger),\n",
    "    Column('entity_one_id', ForeignKey(\"entity.entity_id\", ondelete='CASCADE')),\n",
    "    Column('entity_two_id', ForeignKey(\"entity.entity_id\", ondelete='CASCADE')),\n",
    "    Column('entity_one_word_start', Integer),\n",
    "    Column('entity_one_word_end', Integer),\n",
    "    Column('entity_two_word_start', Integer),\n",
    "    Column('entity_two_word_end', Integer),\n",
    "    Column('sentence_id', ForeignKey(\"sentence.sentence_id\", ondelete='CASCADE'))\n",
    ")\n",
    "\n",
    "metadata.create_all(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is designed to populate the entities table via postgres copy command at the bottom of this notebook. This section first outputs all entities into a single file that will copied onto a postgres table. Make sure you have executed the [pubtator module](https://github.com/greenelab/pubtator) first before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"output\").mkdir(exists_ok=True) # create output folder if doesn'tt exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid_map_df = (\n",
    "    pd.read_csv(\n",
    "        \"../../pubtator/data/pubtator-pmids-to-pmcids.tsv\", \n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "    [[\"PMCID\", \"PMID\"]]\n",
    ")\n",
    "\n",
    "already_seen, last_entity_id, _ = insert_entities(\n",
    "    \"../../pubtator/data/pubtator-central-full-hetnet-tags.tsv.xz\", \n",
    "    pmcid_map = dict(\n",
    "        zip(\n",
    "            pmcid_map_df.PMCID.values, \n",
    "            pmcid_map_df.PMID.values\n",
    "        )\n",
    "    ),\n",
    "    already_seen = already_seen,\n",
    "    entity_id_start=last_entity_id,\n",
    "    output_file=\"output/pubmed_central_entities.tsv\",\n",
    "    seen_entity = seen_entity\n",
    ")\n",
    "\n",
    "print(len(already_seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_seen, last_entity_id, _ = insert_entities(\n",
    "    \"../../pubtator/data/pubtator-central-hetnet-tags.tsv.xz\",\n",
    "    already_seen=already_seen,\n",
    "    output_file=\"output/pubmed_central_entities.tsv\",\n",
    "    entity_id_start=last_entity_id, \n",
    "    skip_documents=True\n",
    ")\n",
    "\n",
    "print(len(already_seen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of documents that have entities\n",
    "(\n",
    "    pd.DataFrame(list(already_seen), columns=[\"document_id\"])\n",
    "    .drop_duplicates()\n",
    "    .to_csv(\n",
    "        \"output/documents_with_entities.tsv\",\n",
    "        sep=\"\\t\", index=False\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes the longest time and requires heavy computation. Uses Spacy to parse sentences and writes features to individual files. Each file will contain every sentence parsed by spacy. Make sure you have enough disk space to run this section of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"output/files\").mkdir(exists_ok=True) # create output folder if doesn't exists\n",
    "n_jobs = 3 #number of processes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Full Text Insertion\")\n",
    "full_path_map = {\n",
    "    \"document_id_path\": \"passage/infon[contains(@key, 'article-id_pmid')]/text()\",\n",
    "    \"passage_path\": \"passage[infon[contains(@key, 'section_type')]]\",\n",
    "    \"section_path\": \"infon[contains(@key, 'section_type')]/text()\",\n",
    "    \"offset_path\" : \"offset/text()\",\n",
    "    \"section_text_path\" : \"text/text()\",\n",
    "}\n",
    "\n",
    "fieldnames = [\n",
    "    \"document_id\",\n",
    "    \"section\", \"position\",\n",
    "    \"text\", \"word\",\n",
    "    \"pos_tag\", \"lemma\",\n",
    "    \"dep\", \"char_offset\"\n",
    "]\n",
    "\n",
    "with Manager() as m:\n",
    "    data_queue = m.JoinableQueue(500000)\n",
    "    jobs = []\n",
    "    \n",
    "    # Start the jobs\n",
    "    for job in range(n_jobs):\n",
    "        p = Process(\n",
    "            target=parse_document, \n",
    "            args=(full_path_map, fieldnames, data_queue)\n",
    "        )\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    # Throw the documents onto the queue\n",
    "    supply_documents(\n",
    "        \"full\", \n",
    "        Path(\"pubtator_central_batch\").rglob(\"*xml\"), # Batch for full text extraction\n",
    "        data_queue\n",
    "    )\n",
    "\n",
    "    # Tell the jobs to end\n",
    "    for job in range(n_jobs):\n",
    "        data_queue.put(None) # poison pill to end the processes\n",
    "\n",
    "    for running_process in jobs:\n",
    "        running_process.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Abstract Insertion\")\n",
    "abs_path_map = {\n",
    "    \"document_id_path\": \"id/text()\",\n",
    "    \"passage_path\":\"passage[infon[contains(@key, 'type')]]\",\n",
    "    \"section_path\": \"infon[contains(@key, 'type')]/text()\",\n",
    "    \"offset_path\": \"offset/text()\",\n",
    "    \"section_text_path\": \"text/text()\"\n",
    "}\n",
    "\n",
    "fieldnames = [\n",
    "    \"document_id\",\n",
    "    \"section\", \"position\",\n",
    "    \"text\", \"word\",\n",
    "    \"pos_tag\", \"lemma\",\n",
    "    \"dep\", \"char_offset\"\n",
    "]\n",
    "\n",
    "tag_generator = ET.iterparse(\n",
    "    lzma.open(\"../../pubtator/data/mar_1/pubtator-central-docs.xml.xz\", \"rb\"), # abstract path\n",
    "    tag=\"document\", \n",
    "    encoding=\"utf-8\",  \n",
    "    recover=True\n",
    ")\n",
    "\n",
    "with Manager() as m:\n",
    "    data_queue = m.JoinableQueue(1000000)\n",
    "    jobs = []\n",
    "    # Start the jobs\n",
    "    for job in range(n_jobs):\n",
    "        p = Process(\n",
    "            target=parse_document, \n",
    "            args=(abs_path_map, fieldnames, data_queue)\n",
    "        )\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    \n",
    "    # Throw the documents onto the queue\n",
    "    supply_documents(\n",
    "        \"abstract\", \n",
    "        tag_generator,\n",
    "        data_queue\n",
    "    )\n",
    "\n",
    "    # Tell the jobs to end\n",
    "    for job in range(n_jobs):\n",
    "        data_queue.put(None) # poison pill to end the processes\n",
    "\n",
    "    for running_process in jobs:\n",
    "        running_process.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Sentence files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section merges all the individual files into a centralized location. This makes it easier to quickly fill sentences into a postgres database (down below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv.field_size_limit(sys.maxsize)\n",
    "with open(\"output/all_pubtator_central_docs.tsv\", \"w\") as outfile:\n",
    "    writer = csv.DictWriter(\n",
    "        outfile,\n",
    "        fieldnames=[\n",
    "            \"sentence_id\", \"document_id\",\n",
    "            \"section\", \"position\", \"text\",\n",
    "            \"word\", \"pos_tag\",\"lemma\",\n",
    "            \"dep\", \"char_offset\"\n",
    "        ],\n",
    "        delimiter=\"\\t\"\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    sentence_id = 1\n",
    "    \n",
    "    for doc_file in tqdm(Path(\"output/files\").rglob(\"*tsv\")):\n",
    "        with open(doc_file,\"r\") as infile:\n",
    "            reader = csv.DictReader(\n",
    "                infile,\n",
    "                fieldnames=[\n",
    "                    \"document_id\", \"section\",\n",
    "                    \"position\", \"text\",\n",
    "                    \"word\",\"pos_tag\",\n",
    "                    \"lemma\", \"dep\",\n",
    "                    \"char_offset\"\n",
    "                ],\n",
    "                delimiter=\"\\t\"\n",
    "            )\n",
    "\n",
    "            for row in reader:\n",
    "                try:\n",
    "                    document_id = int(row['document_id'])\n",
    "                    row['sentence_id'] = sentence_id\n",
    "                    writer.writerow(row)\n",
    "                    sentence_id += 1\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(row['document_id'])\n",
    "                    print(\"Not a valid row skipping!!\")\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert Entities and Sentences into database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section inserts the data into a postgres database. Uses postgres copy function which is amazing at loading large data quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_sql = (\n",
    "    f\"copy public.entity from {Path('output/pubmed_central_entities.tsv').absolute()} \"+\n",
    "    \"delimiter E'\\t' csv HEADER;\"\n",
    ")\n",
    "print(conn.execute(entity_sql))\n",
    "\n",
    "entity_idx_sql = \"select setval('entity_entity_id_seq', (select count(*)+1 from entity), false);\"\n",
    "print(conn.execute(entity_idx_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_sql = (\n",
    "    f\"copy public.sentence from {Path('output/all_pubtator_central_docs.tsv').absolute()} \"+\n",
    "    \"delimiter E'\\t' csv header;\"\n",
    ")\n",
    "print(conn.execute(sentence_sql))\n",
    "\n",
    "sentence_idx_sql = \"select setval('sentence_sentence_id_seq', (select count(*)+1 from sentence), false);\"\n",
    "print(conn.execute(sentence_idx_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section extracts candidates from the loaded sentences above. After loading sentences from each document it uses an interval tree to find multiple entities in the same sentence. These sentences with multiple entities are considered candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_ids = get_sql_result(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT document_id FROM entity;\n",
    "    \"\"\",\n",
    "    conn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/candidates.tsv\", \"w\") as outfile:\n",
    "    \n",
    "    # Create the writer\n",
    "    writer = csv.DictWriter(\n",
    "        outfile, \n",
    "        fieldnames=[\n",
    "            \"candidate_id\",\n",
    "            \"candidate_type\",\"dataset\",\n",
    "            \"entity_one_id\",\"entity_two_id\",\n",
    "            \"entity_one_word_start\",\"entity_one_word_end\",\n",
    "            \"entity_two_word_start\",\"entity_two_word_end\",\n",
    "            \"sentence_id\"\n",
    "        ],\n",
    "        delimiter=\"\\t\"\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    candidate_id = 1\n",
    "    \n",
    "    for document in tqdm.tqdm(document_ids):\n",
    "        entity_rows = get_sql_result(\n",
    "            \"SELECT * FROM entity \" \\\n",
    "            f\"WHERE document_id = {document['document_id']}\",\n",
    "            conn\n",
    "        )\n",
    "        sentence_rows = get_sql_result(\n",
    "            \"SELECT * FROM sentence \" \\\n",
    "            f\"WHERE document_id = {document['document_id']}\",\n",
    "            conn\n",
    "        )\n",
    "        \n",
    "        entity_tree = intervaltree.IntervalTree.from_tuples(\n",
    "            zip(\n",
    "                list(map(lambda x: x['start'], entity_rows)),\n",
    "                list(map(lambda x: x['end'], entity_rows)),\n",
    "                list(\n",
    "                    zip(\n",
    "                        list(map(lambda x: x['entity_id'], entity_rows)),\n",
    "                        list(map(lambda x: x['entity_type'], entity_rows))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for sentence in sentence_rows:\n",
    "            char_list = list(\n",
    "                map(\n",
    "                    int, \n",
    "                    sentence['char_offset'].split(\"|\")\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            potential_candidates = (\n",
    "                entity_tree.overlap(\n",
    "                    begin=min(char_list),\n",
    "                    end=max(char_list)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if len(potential_candidates) < 2:\n",
    "                continue\n",
    "\n",
    "            word_offset = (\n",
    "                dict(\n",
    "                    zip(\n",
    "                        char_list, \n",
    "                        range(len(char_list))\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            for cand in itertools.combinations(potential_candidates, 2):\n",
    "                entity_one_start = cand[0].begin\n",
    "                entity_two_start = cand[1].begin\n",
    "                \n",
    "                if (\n",
    "                    entity_one_start not in word_offset or \n",
    "                    entity_two_start not in word_offset\n",
    "                ):\n",
    "                    continue\n",
    "                \n",
    "                entity_one_end = cand[0].end \n",
    "                entity_two_end = cand[1].end\n",
    "                \n",
    "                if entity_one_end not in word_offset:\n",
    "                    entity_one_end += 1\n",
    "                    \n",
    "                    if entity_one_end not in word_offset:\n",
    "                        continue\n",
    "                        \n",
    "                if entity_two_end not in word_offset:\n",
    "                    entity_two_end += 1\n",
    "                    \n",
    "                    if entity_two_end not in word_offset:\n",
    "                        continue\n",
    "                \n",
    "                writer.writerow({\n",
    "                    \"candidate_id\": candidate_id,\n",
    "                    \"candidate_type\": (\n",
    "                        f\"{cand[0].data[1].lower()}_{cand[1].data[1].lower()}\"\n",
    "                        if cand[0].data[1].lower() < cand[1].data[1].lower()\n",
    "                        else f\"{cand[1].data[1].lower()}_{cand[0].data[1].lower()}\"\n",
    "                    ),\n",
    "                    \"dataset\": -1,\n",
    "                    \"entity_one_id\": cand[0].data[0],\n",
    "                    \"entity_two_id\": cand[1].data[0],\n",
    "                    \"entity_one_word_start\":word_offset[entity_one_start],\n",
    "                    \"entity_one_word_end\":word_offset[entity_one_end],\n",
    "                    \"entity_two_word_start\":word_offset[entity_two_start],\n",
    "                    \"entity_two_word_end\":word_offset[entity_two_end],\n",
    "                    \"sentence_id\": sentence['sentence_id']\n",
    "                })\n",
    "\n",
    "                candidate_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_sql = (\n",
    "    f\"copy public.candidate from {Path('output/candidates.tsv').absolute()} \"+\n",
    "    \"delimiter E'\\t' csv header;\"\n",
    ")\n",
    "print(conn.execute(candidate_sql))\n",
    "\n",
    "candidate_idx_sql = \"select setval('candidate_candidate_id_seq', (select count(*)+1 from candidate), false);\"\n",
    "print(conn.execute(candidate_idx_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot of tables being constructed and filled, so to make your life easier I provided a database view that extracts the necessary information for each candidate. Once this is completed, it should be easy to work with candidates down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disease_gene_view = '''\n",
    "CREATE VIEW disease_gene AS\n",
    "SELECT\n",
    "    CASE WHEN type[1] = 'Disease' THEN cids[1]  ELSE cids[2] end as disease_cid,\n",
    "    CASE WHEN type[1] = 'Disease' THEN cids[2]  ELSE cids[1] end as gene_cid,\n",
    "    candidate_id, \n",
    "    f.sentence_id, \n",
    "    sentence.section,\n",
    "    sentence.text, \n",
    "    sentence.word,\n",
    "    sentence.pos_tag,\n",
    "    sentence.char_offset,\n",
    "    CASE WHEN type[1] = 'Disease' THEN f.entity_one_start ELSE f.entity_two_start end as disease_start,\n",
    "    CASE WHEN type[1] = 'Disease' THEN f.entity_one_end  ELSE f.entity_two_end end as disease_end,\n",
    "    CASE WHEN type[1] = 'Disease' THEN f.entity_two_start  ELSE f.entity_one_start end as gene_start,\n",
    "    CASE WHEN type[1] = 'Disease' THEN f.entity_two_end  ELSE f.entity_one_end end as gene_end\n",
    "    from (\n",
    "         select \n",
    "           array_agg(z.entity_cids) as cids,\n",
    "           array_agg(z.entity_type) as type,\n",
    "           min(candidate_id) as candidate_id, \n",
    "           min(sentence_id) as sentence_id, \n",
    "           min(z.entity_one_word_start) as entity_one_start,\n",
    "           min(z.entity_one_word_end) as entity_one_end,\n",
    "           min(z.entity_two_word_start) as entity_two_start,\n",
    "           min(z.entity_two_word_end) as entity_two_end\n",
    "         from (\n",
    "                select \n",
    "                  entity_cids, \n",
    "                  candidate_id, \n",
    "                  sentence_id, \n",
    "                  candidate.entity_one_word_start,\n",
    "                  candidate.entity_one_word_end,\n",
    "                  candidate.entity_two_word_start,\n",
    "                  candidate.entity_two_word_end,\n",
    "                  entity_type\n",
    "                from entity\n",
    "                inner join candidate on entity.entity_id=candidate.entity_one_id or entity.entity_id=candidate.entity_two_id\n",
    "                where candidate_type = 'disease_gene'\n",
    "                     ) z\n",
    "        group by candidate_id\n",
    "        ) f\n",
    "    inner join sentence on f.sentence_id=sentence.sentence_id;\n",
    "'''\n",
    "conn.execute(disease_gene_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_disease_view = '''\n",
    "CREATE VIEW compound_disease AS\n",
    "SELECT\n",
    "    CASE WHEN type[1] = 'Compound' THEN cids[1] ELSE cids[2] end as compound_cid,\n",
    "    CASE WHEN type[1] = 'Compound' THEN cids[2] ELSE cids[1] end as disease_cid,\n",
    "    candidate_id, \n",
    "    f.sentence_id,\n",
    "    sentence.section,\n",
    "    sentence.text, \n",
    "    sentence.word,\n",
    "    sentence.pos_tag,\n",
    "    sentence.char_offset,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_one_start ELSE f.entity_two_start end as compound_start,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_one_end  ELSE f.entity_two_end end as compound_end,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_two_start  ELSE f.entity_one_start end as disease_start,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_two_end  ELSE f.entity_one_end end as disease_end\n",
    "    from (\n",
    "         select \n",
    "           array_agg(z.entity_cids) as cids,\n",
    "           array_agg(z.entity_type) as type,\n",
    "           min(candidate_id) as candidate_id, \n",
    "           min(sentence_id) as sentence_id, \n",
    "           min(z.entity_one_word_start) as entity_one_start,\n",
    "           min(z.entity_one_word_end) as entity_one_end,\n",
    "           min(z.entity_two_word_start) as entity_two_start,\n",
    "           min(z.entity_two_word_end) as entity_two_end\n",
    "         from (\n",
    "                select \n",
    "                  entity_cids, \n",
    "                  candidate_id, \n",
    "                  sentence_id, \n",
    "                  candidate.entity_one_word_start,\n",
    "                  candidate.entity_one_word_end,\n",
    "                  candidate.entity_two_word_start,\n",
    "                  candidate.entity_two_word_end,\n",
    "                  entity_type\n",
    "                from entity\n",
    "                inner join candidate on entity.entity_id=candidate.entity_one_id or entity.entity_id=candidate.entity_two_id\n",
    "                where candidate_type = 'compound_disease'\n",
    "                     ) z\n",
    "        group by candidate_id\n",
    "        ) f\n",
    "    inner join sentence on f.sentence_id=sentence.sentence_id;\n",
    "'''\n",
    "conn.execute(compound_diseaes_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_gene_view = '''\n",
    "CREATE VIEW compound_gene AS\n",
    "SELECT\n",
    "    CASE WHEN type[1] = 'Compound' THEN cids[1]  ELSE cids[2] end as compound_cid,\n",
    "    CASE WHEN type[1] = 'Compound' THEN cids[2]  ELSE cids[1] end as gene_cid,\n",
    "    candidate_id, \n",
    "    f.sentence_id, \n",
    "    sentence.section,\n",
    "    sentence.text, \n",
    "    sentence.word,\n",
    "    sentence.pos_tag,\n",
    "    sentence.char_offset,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_one_start ELSE f.entity_two_start end as compound_start,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_one_end  ELSE f.entity_two_end end as compound_end,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_two_start  ELSE f.entity_one_start end as gene_start,\n",
    "    CASE WHEN type[1] = 'Compound' THEN f.entity_two_end  ELSE f.entity_one_end end as gene_end\n",
    "    from (\n",
    "         select \n",
    "           array_agg(z.entity_cids) as cids,\n",
    "           array_agg(z.entity_type) as type,\n",
    "           min(candidate_id) as candidate_id, \n",
    "           min(sentence_id) as sentence_id, \n",
    "           min(z.entity_one_word_start) as entity_one_start,\n",
    "           min(z.entity_one_word_end) as entity_one_end,\n",
    "           min(z.entity_two_word_start) as entity_two_start,\n",
    "           min(z.entity_two_word_end) as entity_two_end\n",
    "         from (\n",
    "                select \n",
    "                  entity_cids, \n",
    "                  candidate_id, \n",
    "                  sentence_id, \n",
    "                  candidate.entity_one_word_start,\n",
    "                  candidate.entity_one_word_end,\n",
    "                  candidate.entity_two_word_start,\n",
    "                  candidate.entity_two_word_end,\n",
    "                  entity_type\n",
    "                from entity\n",
    "                inner join candidate on entity.entity_id=candidate.entity_one_id or entity.entity_id=candidate.entity_two_id\n",
    "                where candidate_type = 'compound_gene'\n",
    "                     ) z\n",
    "        group by candidate_id\n",
    "        ) f\n",
    "    inner join sentence on f.sentence_id=sentence.sentence_id;\n",
    "'''\n",
    "conn.execute(compound_gene_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_gene_view = '''\n",
    "CREATE VIEW gene_gene AS\n",
    "SELECT\n",
    "    CASE WHEN entity_one_start < entity_two_start THEN cids[1]  ELSE cids[2] end as gene1_cid,\n",
    "    CASE WHEN entity_one_start < entity_two_start THEN cids[2]  ELSE cids[1] end as gene2_cid,\n",
    "    candidate_id, \n",
    "    f.sentence_id,\n",
    "    sentence.section,\n",
    "    sentence.text, \n",
    "    sentence.word,\n",
    "    sentence.pos_tag,\n",
    "    sentence.char_offset,\n",
    "    CASE WHEN entity_one_start < entity_two_start THEN f.entity_one_start ELSE f.entity_two_start end as gene1_start,\n",
    "    CASE WHEN entity_one_start < entity_two_start THEN f.entity_one_end  ELSE f.entity_two_end end as gene1_end,\n",
    "    CASE WHEN entity_one_start < entity_two_start THEN f.entity_two_start  ELSE f.entity_one_start end as gene2_start,\n",
    "    CASE WHEN entity_one_start < entity_two_start THEN f.entity_two_end ELSE f.entity_one_end end as gene2_end\n",
    "    from (\n",
    "         select \n",
    "           array_agg(z.entity_cids) as cids,\n",
    "           array_agg(z.entity_type) as type,\n",
    "           min(candidate_id) as candidate_id, \n",
    "           min(sentence_id) as sentence_id, \n",
    "           min(z.entity_one_word_start) as entity_one_start,\n",
    "           min(z.entity_one_word_end) as entity_one_end,\n",
    "           min(z.entity_two_word_start) as entity_two_start,\n",
    "           min(z.entity_two_word_end) as entity_two_end\n",
    "         from (\n",
    "                select \n",
    "                  entity_cids, \n",
    "                  candidate_id, \n",
    "                  sentence_id, \n",
    "                  candidate.entity_one_word_start,\n",
    "                  candidate.entity_one_word_end,\n",
    "                  candidate.entity_two_word_start,\n",
    "                  candidate.entity_two_word_end,\n",
    "                  entity_type\n",
    "                from entity\n",
    "                inner join candidate on entity.entity_id=candidate.entity_one_id or entity.entity_id=candidate.entity_two_id\n",
    "                where candidate_type = 'gene_gene'\n",
    "                     ) z\n",
    "        group by candidate_id\n",
    "        ) f\n",
    "    inner join sentence on f.sentence_id=sentence.sentence_id;\n",
    "'''\n",
    "conn.execute(gene_gene_view)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snorkeling_full_text]",
   "language": "python",
   "name": "conda-env-snorkeling_full_text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
